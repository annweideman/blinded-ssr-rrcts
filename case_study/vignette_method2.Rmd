---
title: "Part 2: Illustration of Method 2 for the IAMI Study"
output: html_document
author: "Ann Marie Weideman"
date: "2024-01-22"
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview
This vignette demonstrates how to apply Method 2 presented the manuscript, "Blinded sample size re-estimation for registry-based randomized controlled trials with incomplete event adjudication" by Weideman *et al*. 

Data was generated by using demographics information and outcome proportions reported by the <u>I</u>nfluenza Vaccination <u>A</u>fter <u>M</u>yocardial <u>I</u>nfarction (IAMI) study [@Frobert_2021]. The initial sample size calculations were mirrored after their protocol [@Frobert_2017]. As such, this data is NOT directly derived from the IAMI study and should not be utilized to guide clinical decision making. You are free to utilize this data in any capacity without license or permission. 

The dataset contains treatment arm since it was used to generate associations. However, in practice, the dataset would not include arm since we are blinded to treatment assignment. 

## Data description

The primary endpoint for the IAMI study [@Frobert_2021] was a composite of all-cause mortality, myocardial infarction (MI), or stent thrombosis occurring by 12 months post randomization. 

Consider a classifying event as one or more of the primary events of interest, and a non-classifying event as an alternative form of the event. For the IAMI study, CV death was considered as a classifying event and non-CV death as a non-classifying event. However, the IAMI study reported only 8 non-CV deaths, and so there were not enough adjudicated non-classifying events to develop a prediction model for Method 2. In the Part 1 vignette that illustrates the application of Method 1, we do not stratify by the preliminary classification due to the limited number of non-classifying events. However, in Part 2, we modify the event rates so that there are a larger number of classifying and non-classifying events than reported. In this case, Method 2 can be applied.

The IAMI study team limited their Cox regression to a single predictor: enrolling site. In contrast, our approach also incorporates adjustments for a history of hypertension (HTN) and previous myocardial infarction (MI), given their known association with the primary outcome. Our first logistic regression model used for prediction also adjusts for event times. Typically, event times are incorporated through survival models to account for censoring. However, our initial prediction model excludes censored observations, making the inclusion of these event times suitable. To ensure these event times are on a comparable scale with binary predictors, they are standardized by subtracting the mean and dividing by the standard deviation. Since this is a blinded analysis, we are naive to treatment arm and cannot include it in the models. 

The IAMI study conducted formal adjudication of all primary and secondary endpoints. For the purposes of this demonstration, we assume that their reported outcomes are the adjudicated classifications $Y_1$ and that approximately $50\%$ of the data is adjudicated. We introduce $20\%$ error into the initial classifications $Y_0$ when generating the data.

See the key in the second sheet of `iami2.xlsx` for descriptions of all variables or the header of the data generation `R` script `generate_iami2.R`.

```{r stress}
library(openxlsx)
script_path<-setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
iami2 <- read.xlsx(paste0(script_path, "/data/iami2.xlsx"), sheet = "data")

summary(iami2)

# Event rate in placebo (control) arm
p1C<-mean(iami2$y1_true[iami2$arm==0]>0)
round(p1C,3)

# Event rate in vaccine arm
p1T<-mean(iami2$y1_true[iami2$arm==1]>0)
round(p1T,3)
```

## Estimate sample size at planning stage (stage 0)

```{r}
# Define parameters
n = nrow(iami2) #total sample size at interim analysis
k = 1 # equal number in tx and placebo
alpha = 0.05 # type 1 error
beta = 1-0.8 # 80% power
z_alpha = qnorm(alpha,lower.tail=F)
z_beta = qnorm(beta,lower.tail=F)
t_interim = 1 # time of planned interim analysis in years
```

To demonstrate our methodology, we will analyze the outcome as both a single binary endpoint and as the time to first occurrence of any binary event in the composite endpoint.

The IAMI study assumed that the composite endpoint would be experienced in $10\%$ of participants assigned to placebo and $7.6\%$ of participants assigned to the flu vaccine, corresponding to a hazard ratio (HR) of $0.75$. In this Part 2, we are modifying these anticipated event rates to $20\%$ in those assigned to placebo and $15\%$ in those assigned to vaccine since we are artificially inflating the event rates to illustrate Method 2. Assume that sample size was calculated to achieve $80\%$ statistical power and all testing is one-sided with $\alpha=0.05$ (the authors use two-sided testing).

First, we calculate initial sample size for a single binary endpoint. 

```{r}

# Anticipated event rates
p0T <-  0.15 #vaccine
p0C <- 0.20 #placebo

# Effect size (risk difference)
theta0_bin <-  p0C - p0T

# Weighted event rate
p0 <- (p0C + k*p0T)/(1+k) 

# Number required in control
n0C_bin <- (z_alpha*sqrt((1+k)*p0*(1-p0))+
            z_beta*sqrt(k*p0C*(1-p0C)+p0T*(1-p0T)))^2/(k*theta0_bin^2)

# Number required in treatment
n0T_bin <- k*n0C_bin

# Round up 
n0C_bin<-ceiling(n0C_bin)
n0T_bin<-ceiling(n0T_bin)
n0_bin<-n0C_bin+n0T_bin
n0_bin
```

Next, we calculate initial sample size for a time-to-event endpoint. Common parameter estimates from the calculations for the binary endpoint are recycled. 

```{r}

# Effect size (hazard ratio)
theta0_tte <- log(1-p0T)/log(1-p0C)

# Number of events
d0 <- ((z_alpha+z_beta)*(1+k*theta0_tte)/(sqrt(k)*(1-theta0_tte)))^2 

# Number required in control
n0C_tte <- d0/(k*p0T+p0C) 

# Number required in treatment
n0T_tte <- k*n0C_tte

# Round up 
n0C_tte<-ceiling(n0C_tte)
n0T_tte<-ceiling(n0T_tte)
n0_tte<-n0C_tte+n0T_tte
n0_tte
```

The IAMI study [@Frobert_2021] was terminated approximately halfway through enrollment ($2532$ total participants, $1272$ to influenza vaccine and $1260$ to placebo) due to COVID. Thus, we are treating the final estimates of event rates in each arm as the rates observed at interim analysis for a hypothetical scenario during which the study continues to enroll until completion. Our simulated data yielded primary event rates of $`r round(p1T,3)*100`\%$ for the vaccine group and $`r round(p1C,3)*100`\%$ for the placebo group. 

It's important to note that these event rates have been intentionally increased compared to the rates reported at the end-of-study analysis. This artificial inflation is for the purpose of illustrating Method 2. 

## Re-estimate sample size at interim analysis (stage 1)

We define the function used to carry out Method 2. This function outputs a new dataset of predictions associated with the specified seed. 

```{r}

method2<-function(data, seedling){
  
  set.seed(seedling)
  
  #-----------------------------------------------------------------------------
  # Define the W subsets
  #-----------------------------------------------------------------------------
  # A full dataset that excludes treatment arm
  W<-data

  # A subset of W that includes all adjudicated events and non-events with
  # complete follow-up
  # X = x, Y0 in {-1, 0, 1}, Y1 in {-1, 0, 1}, Z=1
  W_adj<-W[W$adjudicated==1,]
  
  # A subset of W that includes complete adjudications with events
  # X = x, Y0 in {0, 1}, Y1 in {0, 1}, Z = 1
  W_adj_event<-W[W$adjudicated==1 & W$event==1,]
  
  # A subset of W that includes non-events with complete follow-up (so treated
  # as adjudicated)
  # X = x, Y0 = -1, Y1 = -1, Z = 1
  W_adj_nonevent<-W[W$adjudicated==1 & W$event==0,]
  
  # A subset of W that includes all unadjudicated events and non-events 
  # X = x, Y0 in {-1, 0, 1}, Y1 = NA, Z = 0
  W_unadj<-W[W$adjudicated==0,]

  # A subset of W that includes incomplete adjudications with events
  # X = x, Y0 in {0, 1}, Y1 = NA, Z = 0
  W_unadj_event<-W[W$adjudicated==0 & W$event==1,]

  # A subset of W that includes incomplete adjudications with non-events
  # X = x, Y0 = -1, Y1 = NA, Z = 0
  W_unadj_nonevent<-W[W$adjudicated==0 & W$event==0,]

  #-----------------------------------------------------------------------------
  # Step 1: Fit logistic regression to predict missing Y1 in W_unadj_event
  #-----------------------------------------------------------------------------
  
  require(MASS)
  
  # Fit prediction model to patients with adjudicated events
  # For Y0=0
  binom.y0eq0<-glm(y1_obs ~ htn + hxmi + site + t_event_scaled, 
                    family = binomial, 
                    data = W_adj_event[W_adj_event$y0==0,])
  # Sample a new set of coefficients from the multivariate normal distribution
  coef.y0eq0 <- coef(binom.y0eq0)
  cov.y0eq0 <- vcov(binom.y0eq0)
  sampled.coefs.y0eq0 <- mvrnorm(1, mu = coef.y0eq0, Sigma = cov.y0eq0)
  # Update model
  binom.y0eq0$coefficients <- sampled.coefs.y0eq0
  
  # For Y0=1
  binom.y0eq1<-glm(y1_obs ~ htn + hxmi + site + t_event_scaled, 
                    family = binomial, 
                    data = W_adj_event[W_adj_event$y0==1,])
  # Sample a new set of coefficients from the multivariate normal distribution
  coef.y0eq1 <- coef(binom.y0eq1)
  cov.y0eq1 <- vcov(binom.y0eq1)
  sampled.coefs.y0eq1 <- mvrnorm(1, mu = coef.y0eq1, Sigma = cov.y0eq1)
  # Update model
  binom.y0eq1$coefficients <- sampled.coefs.y0eq1
  
  # Predict outcomes for patients with unadjudicated events
  prob.binom.y0eq0<-predict(binom.y0eq0,
                            newdata=W_unadj_event[W_unadj_event$y0==0,],
                            type="response")
  pred.binom.y0eq0<-rbinom(nrow(W_unadj_event[W_unadj_event$y0==0,]),
                           1,prob.binom.y0eq0)
  prob.binom.y0eq1<-predict(binom.y0eq1,
                            newdata=W_unadj_event[W_unadj_event$y0==1,],
                            type="response")
  pred.binom.y0eq1<-rbinom(nrow(W_unadj_event[W_unadj_event$y0==1,]),
                           1,prob.binom.y0eq1)
  
  #-----------------------------------------------------------------------------
  # Step 2: Replace all Y0=-1 and Y1=-1 in W_adj_nonevent  
  # with Y0=0 and Y1=0 to correspond to censored observations.
  # Also, replace all Y0=-1 in W_unadj_nonevent with Y0=0 to correspond to 
  # censored observations
  #-----------------------------------------------------------------------------
  
  W_adj_nonevent$y0[W_adj_nonevent$y0==-1]<-0
  W_adj_nonevent$y1_obs[W_adj_nonevent$y1_obs==-1]<-0
  W_unadj_nonevent$y0<-0

  #-----------------------------------------------------------------------------
  # Step 3: Fit Cox proportional hazards model to predict missing Y1 for 
  #         event-free participants who are still under observation
  #-----------------------------------------------------------------------------

  W_hat_unadj_event<-W_unadj_event 
  # Replace y1_obs with predicted values from step 1
  W_hat_unadj_event[W_hat_unadj_event$y0==0,]$y1_obs<-pred.binom.y0eq0
  W_hat_unadj_event[W_hat_unadj_event$y0==1,]$y1_obs<-pred.binom.y0eq1
  W_hat0<-rbind(W_adj_event, W_adj_nonevent, W_hat_unadj_event)

  library(survival)
  # Cox regression
  cox <- coxph(Surv(t_eligible, y1_obs) ~ htn + hxmi + site, data = W_hat0)
  
  # Sample a new set of coefficients from the multivariate normal distribution
  coef.cox <- coef(cox)
  cov.cox <- vcov(cox)
  sampled.coefs.cox <- mvrnorm(1, mu = coef.cox, Sigma = cov.cox)
  # Update model
  cox$coefficients <- sampled.coefs.cox
  
  # Predict relative risks for W_unadj_nonevent
  rr <- predict(cox, newdata = W_unadj_nonevent, type = "risk")

  # Estimate the baseline hazard function using Breslow's estimator
  baseline_h <- basehaz(cox, centered = FALSE)
  
  # Function to interpolate baseline hazard for each time point
  interpolate_hazard <- function(time_points, baseline_h) {
    sapply(time_points, function(time) {
      if(any(baseline_h$time >= time)) {
        idx <- which.max(baseline_h$time[baseline_h$time <= time])
        return(baseline_h$hazard[idx])
      } else {
        return(NA)
      }
    })
  }

  # Interpolate the baseline hazard for each time point in W_unadj_nonevent
  interpolated_baseline_h <- interpolate_hazard(W_unadj_nonevent$t_eligible, 
                                                 baseline_h)

  # Calculate absolute hazard rates
  h <- interpolated_baseline_h * rr

  # Calculate conditional probability of an event between time t_eligible and 
  # t_interim
  prob.cox<-1-exp(-h*(t_interim-W_unadj_nonevent$t_eligible))
  pred.cox<-rbinom(nrow(W_unadj_nonevent),1,prob.cox)
  
  #-----------------------------------------------------------------------------
  # Step 4: Combine datasets and output
  #-----------------------------------------------------------------------------
  W_unadj_nonevent$y1_obs<-pred.cox
  W_hat<-rbind(W_hat0, W_unadj_nonevent)
  
  return(W_hat)
  
}

```

Within the function, the adjudicated outcomes are sampled from a binomial distribution with parameter equal to the predicted probabilities. Since a single random sample of binomial random variates may not accurately represent the underlying distribution for the adjudicated outcomes, we repeat the routine a total of 100 times, using a different seed within each routine, and combine the resulting datasets.

```{r, message=F, warning=F}

combo_data<-data.frame()
for(i in 1:100){
  out_data<-method2(iami2,seedling=i)  
  combo_data<-rbind(combo_data,out_data)
}

# Estimate pooled event rates across combined datasets
adj_ids<-which(combo_data$adjudicated==1)
unadj_ids<-which(combo_data$adjudicated==0)
 
# Truth
p1_true<-mean(iami2$y1_true>0) 
# Method 2
p1_method2<-mean(combo_data$y1_obs) 
# Complete case
p1_complete_case<-mean(combo_data$y1_obs[adj_ids])
# Carry forward 
p1_carry_forward<-mean(c(combo_data$y1_obs[adj_ids],combo_data$y0[unadj_ids]))

est_n1_bin<-function(p1_hat){
  n1_hat<-(z_alpha+z_beta)^2*(1+k)^2*p1_hat*(1-p1_hat)/(k*theta0_bin^2)
  return(ceiling(n1_hat))
}

est_n1_tte<-function(p1_hat){
    n1_hat<-d0/p1_hat
    return(ceiling(n1_hat))
}
  
# True re-estimated n
n1_bin_true <- est_n1_bin(p1_true)
n1_tte_true <- est_n1_tte(p1_true)

# Method 1 re-estimated n
n1_bin_method2 <- est_n1_bin(p1_method2)
n1_tte_method2 <- est_n1_tte(p1_method2)
  
# Complete case re-estimated n
n1_bin_complete_case <- est_n1_bin(p1_complete_case)
n1_tte_complete_case <- est_n1_tte(p1_complete_case)
  
# Carry forward re-estimated n
n1_bin_carry_forward <- est_n1_bin(p1_carry_forward)
n1_tte_carry_forward <- est_n1_tte(p1_carry_forward)
  
n1_hat<-c(n1_bin_true, n1_bin_method2, n1_bin_complete_case, n1_bin_carry_forward,
          n1_tte_true, n1_tte_method2, n1_tte_complete_case, n1_tte_carry_forward)

```

The sample size estimates no longer coincide for binary and time-to-event endpoints. We now observe that, for the binary endpoint, the required sample size exceeds our initial estimate (originally $`r n0_bin`$, now revised to $`r n1_bin_method2`$), while, for the time-to-event endpoint, it is smaller than the initial estimate (originally $`r n0_tte`$, now revised to $`r n1_tte_method2`$). Based on the criteria outlined in the main text, the study will proceed with the initially planned sample size for the time-to-event endpoint, as the re-calculated sample size is smaller than the original estimate.

In this specific example, $\widehat{p}_1 = `r round(p1_method2,3)`$ is larger than the anticipated pooled event rate of $p_0 = `r round(p0,3)`$ and deviates from the truth of $\widehat{p}_1 = `r round(p1_true,3)`$ by $`r abs(round(p1_method2,3) - round(p1_true,3))`$. In the binomial case, as $\widehat{p}_1$ approaches 0.5, the term $\widehat{p}_1(1-\widehat{p}_1)$ in the numerator of the formula for $\widehat{n}_1$ results in an increase in the required sample size. In contrast, in a time-to-event scenario, any increase in $\widehat{p}_1$ causes the denominator of the formula for $\widehat{n}_1$ to increase. Consequently, since $\widehat{p}_1$ is greater than the original calculation of $p_0$, a smaller re-estimated sample size is sufficient to achieve the necessary number of events.

Note that the complete case estimate of $`r round(p1_complete_case,3)`$ and the carry forward estimate of $`r round(p1_carry_forward,3)`$ are both biased downward compared to the true value of $`r round(p1_true,3)`$.

Finally, the $70\%$ CIs for the re-estimated sample size can be generated using the closed form expressions defined in the main text. 

```{r, message=F}

# Functions to compute closed form CIs
# Binary endpoint
binary_ci<-function(n1_hat, p1_hat, alpha){
  z_alpha2<-qnorm(alpha/2, lower.tail=F)
  se<-((z_alpha+z_beta)^2*(1+k)^2/(k*theta0_bin^2)*
                            sqrt(p1_hat*(1-p1_hat)*(1-2*p1_hat)^2/n))
  lb<-ceiling(n1_hat-z_alpha2*se)
  ub<-ceiling(n1_hat+z_alpha2*se)
  
  return(c(lb,ub))
}

# Time-to-event endpoint
tte_ci<-function(n1_hat, p1_hat, alpha){
  z_alpha2<-qnorm(alpha/2, lower.tail=F)
  se<-sqrt((1-p1_hat)/(n*p1_hat))
  lb<-ceiling(exp(log(n1_hat)-z_alpha2*se))
  ub<-ceiling(exp(log(n1_hat)+z_alpha2*se))
  
  return(c(lb,ub))
}

# CI limits for closed form CIs
Lower<-ceiling(c(binary_ci(n1_bin_true, p1_true, 0.30)[1],
                 binary_ci(n1_bin_method2, p1_method2, 0.30)[1],
                 binary_ci(n1_bin_complete_case, p1_complete_case, 0.30)[1],
                 binary_ci(n1_bin_carry_forward, p1_carry_forward, 0.30)[1],
                 tte_ci(n1_tte_true, p1_true, 0.30)[1],
                 tte_ci(n1_tte_method2, p1_method2, 0.30)[1],
                 tte_ci(n1_tte_complete_case, p1_complete_case, 0.30)[1],
                 tte_ci(n1_tte_carry_forward, p1_carry_forward, 0.30)[1]))
Upper<-ceiling(c(binary_ci(n1_bin_true, p1_true, 0.30)[2],
                 binary_ci(n1_bin_method2, p1_method2, 0.30)[2],
                 binary_ci(n1_bin_complete_case, p1_complete_case, 0.30)[2],
                 binary_ci(n1_bin_carry_forward, p1_carry_forward, 0.30)[2],
                 tte_ci(n1_tte_true, p1_true, 0.30)[2],
                 tte_ci(n1_tte_method2, p1_method2, 0.30)[2],
                 tte_ci(n1_tte_complete_case, p1_complete_case, 0.30)[2],
                 tte_ci(n1_tte_carry_forward, p1_carry_forward, 0.30)[2]))

```

## Plot results

```{r, message=F, warning=F}
library(ggplot2)
library(dplyr)

data <- data.frame(
  Method = rep(c("Truth", "Method 2", "Complete case", "Carry forward"), 2),
  Type = factor(rep(c("Binary", "Time-to-event"), each = 4)),
  n1_hat = n1_hat, # re-estimated sample sizes
  Lower = Lower, # lower limits of CIs
  Upper = Upper # upper limits of CIs
)

# Re-order levels for plot
data$Method <- factor(data$Method, 
                      levels = c("Truth", "Method 2", 
                                 "Complete case", "Carry forward"))

# Add a column for a vertical line denoting the truth
data$VLine <- ifelse(data$Type == "Binary", n1_hat[1], n1_hat[5])

# Define color-blind friendly colors for each method
method_colors <- c(
  "Truth" = "#E69F00",  # Orange
  "Method 2" = "#56B4E9",  # Sky Blue
  "Complete case" = "#009E73",  # Bluish Green
  "Carry forward" = "#F0E442"   # Yellow
)

plot <- ggplot(data, aes(x = Method, y = n1_hat, color = Method)) +
  geom_point(size = 3) +  
  geom_abline(aes(intercept = VLine, slope = 0), linetype = "dashed") +
  geom_errorbar(aes(ymin = Lower, ymax = Upper), width = 0.2, size = 1) + 
  theme_minimal() +
  labs(x="", y = "Re-estimated sample size at interim analysis (70% CI)") +
  coord_flip() + 
  scale_color_manual(values = method_colors) +
  facet_wrap(~ Type, scales = "free", ncol = 1) +
  theme(plot.margin = margin(1, 1, 1, 1, "cm"), 
    strip.text = element_text(face = "bold", size = rel(1.5), margin = margin(b = 8)), 
    legend.position = "bottom", 
    legend.justification = "right",
    legend.direction = "horizontal",
    legend.box.just = "right",
    axis.title.x = element_text(margin = margin(t = 20), size=rel(1.5)),
    axis.title.y = element_text(size = rel(1.4)),  # Increase Y axis title size
    axis.text.x = element_text(size = rel(1.5)),   # Increase X axis text size
    axis.text.y = element_text(size = rel(1.5)),   # Increase Y axis text size
    legend.text = element_text(size = rel(1.2)),   # Increase legend text size
    legend.title = element_text(size = rel(1.3))   # Increase legend title size
  ) +
  guides(color = guide_legend(title = "Method")) 

print(plot)
```

## References
